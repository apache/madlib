# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

"""
@file knn.py_in

@brief knn: K-Nearest Neighbors for regression and classification

@namespace knn

"""

import plpy
from math import floor
from math import log
from utilities.control import MinWarning
from utilities.utilities import _assert
from utilities.utilities import add_postfix
from utilities.utilities import py_list_to_sql_string, is_valid_psql_type
from utilities.utilities import unique_string
from utilities.utilities import NUMERIC, ONLY_ARRAY
from utilities.utilities import is_valid_psql_type
from utilities.utilities import is_pg_major_version_less_than
from utilities.validate_args import array_col_has_no_null
from utilities.validate_args import cols_in_tbl_valid
from utilities.validate_args import drop_tables
from utilities.validate_args import get_cols
from utilities.validate_args import get_expr_type
from utilities.validate_args import input_tbl_valid, output_tbl_valid
from utilities.validate_args import is_col_array
from utilities.validate_args import is_var_valid
from utilities.validate_args import quote_ident

MAX_WEIGHT_ZERO_DIST = 1e6


def knn_validate_src(schema_madlib, point_source, point_column_name, point_id,
                     label_column_name, test_source, test_column_name,
                     test_id, output_table, k, output_neighbors, fn_dist,
                     **kwargs):
    input_tbl_valid(point_source, 'kNN')
    input_tbl_valid(test_source, 'kNN')
    output_tbl_valid(output_table, 'kNN')

    _assert(label_column_name or output_neighbors,
            "kNN error: Either label_column_name or "
            "output_neighbors has to be inputed.")

    if label_column_name and label_column_name.strip():
        cols_in_tbl_valid(point_source, [label_column_name], 'kNN')

    _assert(is_var_valid(point_source, point_column_name),
            "kNN error: {0} is an invalid column name or expression for point_column_name param".format(point_column_name))
    point_col_type = get_expr_type(point_column_name, point_source)
    _assert(is_valid_psql_type(point_col_type, NUMERIC | ONLY_ARRAY),
            "kNN Error: Feature column or expression '{0}' in train table is not"
            " an array.".format(point_column_name))

    _assert(is_var_valid(test_source, test_column_name),
            "kNN error: {0} is an invalid column name or expression for test_column_name param".format(test_column_name))
    test_col_type = get_expr_type(test_column_name, test_source)
    _assert(is_valid_psql_type(test_col_type, NUMERIC | ONLY_ARRAY),
            "kNN Error: Feature column or expression '{0}' in test table is not"
            " an array.".format(test_column_name))

    cols_in_tbl_valid(point_source, [point_id], 'kNN')
    cols_in_tbl_valid(test_source, [test_id], 'kNN')

    if not array_col_has_no_null(point_source, point_column_name):
        plpy.error("kNN Error: Feature column '{0}' in train table has some"
                   " NULL values.".format(point_column_name))
    if not array_col_has_no_null(test_source, test_column_name):
        plpy.error("kNN Error: Feature column '{0}' in test table has some"
                   " NULL values.".format(test_column_name))

    if k <= 0:
        plpy.error("kNN Error: k={0} is an invalid value, must be greater "
                   "than 0.".format(k))

    bound = plpy.execute("SELECT {k} <= count(*) AS bound FROM {tbl}".
                         format(k=k, tbl=point_source))[0]['bound']
    if not bound:
        plpy.error("kNN Error: k={0} is greater than number of rows in"
                   " training table.".format(k))

    if label_column_name:
        col_type = get_expr_type(label_column_name, point_source).lower()
        if col_type not in ['integer', 'double precision', 'float', 'boolean']:
            plpy.error("kNN error: Invalid data type '{0}' for"
                       " label_column_name in table '{1}'.".
                       format(col_type, point_source))

    col_type_test = get_expr_type(test_id, test_source).lower()
    if col_type_test not in ['integer']:
        plpy.error("kNN Error: Invalid data type '{0}' for"
                   " test_id column in table '{1}'.".
                   format(col_type_test, test_source))

    if fn_dist:
        fn_dist = fn_dist.lower().strip()
        dist_functions = set(["{0}.{1}".format(schema_madlib, dist) for dist in
                              ('dist_norm1', 'dist_norm2',
                               'squared_dist_norm2', 'dist_angle',
                               'dist_tanimoto')])

        profunc = ("proisagg = TRUE" if is_pg_major_version_less_than(schema_madlib, 11)
              else "prokind = 'a'")

        is_invalid_func = plpy.execute("""
            SELECT prorettype != 'DOUBLE PRECISION'::regtype OR
                   {profunc} AS OUTPUT
            FROM pg_proc
            WHERE oid='{fn_dist}(DOUBLE PRECISION[], DOUBLE PRECISION[])'::regprocedure;
            """.format(fn_dist=fn_dist, profunc=profunc))[0]['output']

        if is_invalid_func or (fn_dist not in dist_functions):
            plpy.error("KNN error: Distance function has invalid signature "
                       "or is not a simple function.")

    return k
# ------------------------------------------------------------------------------


def kd_tree(schema_madlib, source_table, output_table, point_column_name, depth, **kwargs):

    with MinWarning("error"):

        validate_kd_tree(source_table, output_table, point_column_name, depth)
        num_features = plpy.execute("SELECT array_upper({point_column_name},1) AS num FROM {source_table}".format(**locals()))[0]['num']

        clauses = []
        cutoffs = []
        clause_counter = 0
        current_feature = 1
        clauses.append(" WHERE 1=1 ")
        for i in range(0, depth):
            for j in range(0, pow(2,i)):
                clause = clauses[clause_counter]
                cutoff_sql = """SELECT percentile_disc(0.5) within group (
                            ORDER BY {point_column_name}[{current_feature}]) AS cutoff
                        FROM {source_table} {clause}""".format(**locals())

                cutoff = plpy.execute(cutoff_sql)[0]['cutoff']
                cutoffs.append(cutoff if cutoff is not None else "NULL")
                clause_counter = clause_counter + 1
                clauses.append(clause + "AND {point_column_name}[{current_feature}] < {cutoff} ".format(**locals()))
                clauses.append(clause + "AND {point_column_name}[{current_feature}] >= {cutoff} ".format(**locals()))
            current_feature = current_feature % num_features + 1


        plpy.execute("CREATE TABLE {output_table}_tree (tree DOUBLE PRECISION[]);".format(**locals()))

        plpy.execute("""INSERT INTO {0} (tree) VALUES ('{{ {1} }}')""".format(
            output_table+"_tree", " , ".join(map(str,cutoffs))))

        output_sql = " CREATE TABLE {output_table} AS SELECT *, CASE "
        num_leaves = pow(2,depth)
        for i in range(0, num_leaves):
            cond = clauses.pop()[14:]

            output_sql += "WHEN {0} THEN {1}::INTEGER \n".format(cond, num_leaves-i-1)
        output_sql = (output_sql + "END AS case FROM {source_table}").format(**locals())
        plpy.execute(output_sql)


def validate_kd_tree(source_table, output_table, point_column_name, depth):

    input_tbl_valid(source_table, 'kd_tree')
    output_tbl_valid(output_table, 'kd_tree')
    output_tbl_valid(output_table+"_tree", 'kd_tree')

    _assert(is_var_valid(source_table, point_column_name),
            "kd_tree error: {0} is an invalid column name or expression for point_column_name param".format(point_column_name))
    point_col_type = get_expr_type(point_column_name, source_table)
    _assert(is_valid_psql_type(point_col_type, NUMERIC | ONLY_ARRAY),
            "kNN Error: Feature column or expression '{0}' in train table is not"
            " an array.".format(point_column_name))
    if depth <= 0:
        plpy.error("kNN Error: depth={0} is an invalid value, must be greater "
                   "than 0.".format(depth))

def knn_tree(schema_madlib, kd_out, point_source, point_column_name, point_id,
        label_column_name, test_source, test_column_name, test_id, interim_table,
        in_k, output_neighbors, fn_dist, weighted_avg, label_out,
        comma_label_out_alias, label_name, train, train_id, dist_inverse, **kwargs):

    with MinWarning("error"):

        tree_model = kd_out + "_tree"
        num_features = plpy.execute("SELECT array_upper({point_column_name},1) AS num FROM {test_source}".format(**locals()))[0]['num']

        tree = plpy.execute("SELECT * FROM {0}".format(tree_model))[0]['tree']
        num_leaves = len(tree)+1

        depth = int(log(num_leaves, 2))

        clauses = []
        clause_counter = 0
        tree_counter = 0
        current_feature = 1
        borders = {}
        borders_table = unique_string() + "_borders"
        for i in range(0,num_features):
            borders[i] = []

        for i in range(0, num_leaves):
            clauses.append(" 1=1 ")

        for i in range(0,depth):
            repeat_num = num_leaves / (pow(2,i+1))

            for j in range(0, pow(2,i)):
                cutoff = tree[tree_counter]
                borders[current_feature-1].append(cutoff)
                for k in range(repeat_num):
                    clause = clauses.pop(0)
                    clause += " AND {point_column_name}[{current_feature}] < {cutoff} ".format(**locals())
                    clauses.append(clause)
                for k in range(repeat_num):
                    clause = clauses.pop(0)
                    clause += " AND {point_column_name}[{current_feature}] >= {cutoff} ".format(**locals())
                    clauses.append(clause)
                tree_counter += 1

            current_feature = current_feature % num_features + 1

        plpy.execute("DROP TABLE IF EXISTS {borders_table}".format(**locals()))
        create_sql = "CREATE TABLE {borders_table} ("
        create_sql_cons = []
        for i in range(0,num_features):
            create_sql_cons.append(" dim{0} DOUBLE PRECISION[] ".format(str(i)))
        create_sql += " , ".join(create_sql_cons) + ") "

        plpy.execute(create_sql.format(**locals()))

        insert_fields_sql_cons = []
        for i in range(0,num_features):
            insert_fields_sql_cons.append(" dim{0} ".format(str(i)))

        insert_sql = "INSERT INTO {0} ({1}) VALUES \n (".format(
                            borders_table, " , ".join(insert_fields_sql_cons))
        insert_values_sql_cons = []
        for i in range(0,num_features):

            insert_values_sql_cons.append(" ARRAY{0}::double precision[] ".format(str(borders[i])))
        insert_sql += " , ".join(insert_values_sql_cons) + " )"
        plpy.execute(insert_sql)

        test_view = unique_string()+"_test_view"

        plpy.execute("DROP VIEW IF EXISTS {test_view}".format(**locals()))
        test_view_sql = " CREATE VIEW {test_view} AS SELECT *, CASE ".format(**locals())

        for i in range(0, num_leaves):
            test_view_sql += " WHEN {0} THEN {1}::INTEGER ".format(clauses[i], i)

        test_view_sql += "END AS case FROM {0}".format(test_source)

        plpy.execute(test_view_sql)

        sql = """ CREATE TABLE {interim_table} AS
            SELECT * FROM (
                SELECT row_number() over (partition by {test_id} ORDER BY dist) AS r,
                       {test_id}, {train_id}, dist,
                       CASE WHEN dist = 0.0 THEN {max_weight_zero_dist}
                                 ELSE 1.0 / dist
                            END AS {dist_inverse}
                            {comma_label_out_alias}
                FROM (
                    SELECT {train}.case AS tr_case, test.case AS test_case,
                           {train}.{point_id} AS {train_id},test.{test_id} AS {test_id},
                           {fn_dist}({train}.data,test.data) AS dist
                           {label_out}
                    FROM {kd_out} AS {train} INNER JOIN {test_view} AS test ON {train}.case = test.case
                    ) q1
                )q2
            WHERE r <= {in_k}""".format(max_weight_zero_dist=MAX_WEIGHT_ZERO_DIST,
                **locals())
        plpy.execute(sql)

        max_dist_sql = """SELECT {test_id}, max(dist) AS dists
                          FROM {interim_table} GROUP BY {test_id} ORDER BY {test_id}""".format(**locals())
        redo = []
        tmp_compare = unique_string(desp="tmp_compare1")

        for i in range(0,min(num_features,depth)):

            plpy.execute("DROP TABLE IF EXISTS {tmp_compare}".format(**locals()))
            plpy.execute("""
            CREATE TABLE {tmp_compare} AS SELECT {test_id}, max, ({test_column_name}[{i}+1] - max) * ({test_column_name}[{i}+1] - max) AS border_dists
            FROM (
                SELECT {test_id}, {test_column_name}, max(dim{i}) AS max
                FROM {test_view} , (
                    SELECT unnest(dim{i}) AS dim{i} FROM {borders_table})q1
                WHERE  {test_column_name}[{i}+1] >= dim{i} group by {test_id},{test_column_name}) q2; """.format(**locals()))

            plpy.execute("""
            INSERT INTO {tmp_compare} (SELECT {test_id}, min, ({test_column_name}[{i}+1] - min) * ({test_column_name}[{i}+1] - min) AS  border_dists
            FROM (
                SELECT {test_id}, {test_column_name}, min(dim{i}) AS min
                FROM {test_view} , (
                    SELECT unnest(dim{i}) AS dim{i} FROM {borders_table})q1
                WHERE  {test_column_name}[{i}+1] < dim{i} group by {test_id},{test_column_name}) q2)""".format(**locals()))

            redo_sql = """ SELECT array_agg(q2.{test_id}) AS {test_id}, array_agg(dists) AS dist,
                                  array_agg(border_dists) AS border
                        FROM {tmp_compare} INNER JOIN ({max_dist_sql})q2 ON (q2.{test_id} = {tmp_compare}.{test_id})
                        WHERE q2.dists > border_dists
                    """.format(**locals())

            to_redo = plpy.execute(redo_sql)[0]
            if to_redo["id"] is not None:
                for j in to_redo["id"]:
                    if j not in redo:
                        redo.append(j)
            tmp_compare = unique_string(desp="tmp_compare2")

        if len(redo) > 0:

            tmp_view = unique_string() + "_tmp_view"
            plpy.execute("DROP VIEW IF EXISTS {tmp_view}".format(**locals()))
            plpy.execute(""" CREATE VIEW {tmp_view} AS
                SELECT * FROM {test_source}
                WHERE id=ANY(ARRAY{redo})""".format(**locals()))

            plpy.execute("""
                DELETE FROM {interim_table} WHERE id=ANY(ARRAY{redo})
                """.format(**locals()))

            plpy.execute("""
                INSERT INTO {interim_table}
                    (r, {test_id}, {train_id}, dist, {dist_inverse} {comma_label_out_alias})
                SELECT * FROM
                    (SELECT row_number() OVER
                                (PARTITION BY {test_id} ORDER BY dist) AS r,
                            {test_id}, {train_id},  dist,
                            CASE WHEN dist = 0.0 THEN {max_weight_zero_dist}
                                 ELSE 1.0 / dist
                            END AS {dist_inverse}
                            {comma_label_out_alias}
                     FROM (
                        SELECT {train}.{point_id} AS {train_id},test.{test_id} AS {test_id},
                                {fn_dist}({train}.data,test.data) AS dist
                                {label_out}
                        FROM (
                            SELECT {point_id} , {point_column_name} AS data {label_name}
                            FROM {point_source}
                            ) AS {train},
                            (
                            SELECT {test_id} ,{test_column_name} AS data FROM {tmp_view}
                            ) AS test
                        )q1
                    )q2
                WHERE r <= {in_k} """.format(max_weight_zero_dist=MAX_WEIGHT_ZERO_DIST,
                                          **locals()))

        drop_tables([borders_table, tmp_compare])

def knn(schema_madlib, point_source, point_column_name, point_id,
        label_column_name, test_source, test_column_name, test_id, output_table,
        k, output_neighbors, fn_dist, weighted_avg, use_kdtree, **kwargs):
    """
        KNN function to find the K Nearest neighbours
        Args:
            @param schema_madlib        Name of the Madlib Schema
            @param point_source         Training data table
            @param point_column_name    Name of the column with training data
                                        or expression that evaluates to a
                                        numeric array
            @param point_id             Name of the column having ids of data
                                        point in train data table
                                        points.
            @param label_column_name    Name of the column with labels/values
                                        of training data points.
            @param test_source          Name of the table containing the test
                                        data points.
            @param test_column_name     Name of the column with testing data
                                        points or expression that evaluates to a
                                        numeric array
            @param test_id              Name of the column having ids of data
                                        points in test data table.
            @param output_table         Name of the table to store final
                                        results.
            @param k                    default: 1. Number of nearest
                                        neighbors to consider
            @output_neighbours          Outputs the list of k-nearest neighbors
                                        that were used in the voting/averaging.
            @param fn_dist              Distance metrics function. Default is
                                        squared_dist_norm2. Following functions
                                        are supported :
                                        dist_norm1 , dist_norm2,squared_dist_norm2,
                                        dist_angle , dist_tanimoto
                                        Or user defined function with signature
                                        DOUBLE PRECISION[] x, DOUBLE PRECISION[] y -> DOUBLE PRECISION
            @param weighted_avg         Calculates the Regression or classication of k-NN using
                                        the weighted average method.
    """
    with MinWarning('warning'):
        output_neighbors = True if output_neighbors is None else output_neighbors
        if k is None:
            k = 1
        knn_validate_src(schema_madlib, point_source,
                         point_column_name, point_id, label_column_name,
                         test_source, test_column_name, test_id,
                         output_table, k, output_neighbors, fn_dist)

        # Unique Strings
        x_temp_table = unique_string(desp='x_temp_table')
        y_temp_table = unique_string(desp='y_temp_table')
        label_col_temp = unique_string(desp='label_col_temp')
        test_id_temp = unique_string(desp='test_id_temp')
        train = unique_string(desp='train')
        test = unique_string(desp='test')
        p_col_name = unique_string(desp='p_col_name')
        t_col_name = unique_string(desp='t_col_name')
        dist = unique_string(desp='dist')
        train_id = unique_string(desp='train_id')
        dist_inverse = unique_string(desp='dist_inverse')
        r = unique_string(desp='r')

        if not fn_dist:
            fn_dist = '{0}.squared_dist_norm2'.format(schema_madlib)

        fn_dist = fn_dist.lower().strip()
        interim_table = unique_string(desp='interim_table')

        pred_out = ""
        knn_neighbors = ""
        label_out = ""
        cast_to_int = ""
        view_def = ""
        view_join = ""
        view_grp_by = ""

        if label_column_name:
            label_column_type = get_expr_type(
                label_column_name, point_source).lower()
            if label_column_type in ['boolean', 'integer', 'text']:
                is_classification = True
                cast_to_int = '::INTEGER'
            else:
                is_classification = False

            if is_classification:
                if weighted_avg:
                    # This view is to calculate the max value of sum of the 1/distance grouped by label and Id.
                    # And this max value will be the prediction for the
                    # classification model.
                    view_def = """
                        WITH vw AS (
                            SELECT DISTINCT ON({test_id_temp})
                                {test_id_temp},
                                last_value(data_sum) OVER (
                                    PARTITION BY {test_id_temp}
                                    ORDER BY data_sum, {label_col_temp}
                                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
                                    ) AS data_dist ,
                                last_value({label_col_temp}) OVER (
                                    PARTITION BY {test_id_temp}
                                    ORDER BY data_sum, {label_col_temp}
                                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
                                    ) AS {label_col_temp}
                            FROM   (
                                SELECT
                                    {test_id_temp},
                                    {label_col_temp},
                                    sum({dist_inverse}) data_sum
                                FROM {interim_table}
                                GROUP BY {test_id_temp},
                                         {label_col_temp}
                            ) a
                            -- GROUP BY {test_id_temp} , {label_col_temp}
                        )
                        """.format(**locals())
                    # This join is needed to get the max value of predicion
                    # calculated above
                    view_join = (" JOIN vw ON knn_temp.{0} = vw.{0}".
                                 format(test_id_temp))
                    view_grp_by = ", vw.{0}".format(label_col_temp)
                    pred_out = ", vw.{0}".format(label_col_temp)
                else:
                    pred_out = ", {0}.mode({1})".format(
                        schema_madlib, label_col_temp)
            else:
                if weighted_avg:
                    pred_out = (", sum({0} * {dist_inverse}) / sum({dist_inverse})".
                                format(label_col_temp, dist_inverse=dist_inverse))
                else:
                    pred_out = ", avg({0})".format(label_col_temp)

            pred_out += " AS prediction"
            label_out = (", {train}.{label_column_name}{cast_to_int}"
                         " AS {label_col_temp}").format(**locals())
            comma_label_out_alias = ', ' + label_col_temp
            label_name = ", {label_column_name}".format(
                label_column_name=label_column_name)

        else:
            pred_out = ""
            label_out = ""
            comma_label_out_alias = ""
            label_name = ""

        # interim_table picks the 'k' nearest neighbors for each test point
        if output_neighbors:
            knn_neighbors = (", array_agg(knn_temp.{train_id} ORDER BY "
                             "knn_temp.{dist_inverse} DESC) AS k_nearest_neighbours ").format(**locals())
        else:
            knn_neighbors = ''

        if use_kdtree:
            #CALL new funcs

            size = plpy.execute("SELECT count(*) AS count FROM {0}".format(point_source))[0]['count']
            depth = int(floor(log(size, 10)))
            if depth < 1:
                use_kdtree = False
            else:
                if depth > 3:
                    depth = 3
                kd_output_table = unique_string(desp='kd_tree')
                kd_tree(schema_madlib, point_source, kd_output_table, point_column_name, depth)
                knn_tree(schema_madlib, kd_output_table, point_source,
                         point_column_name, point_id, label_column_name,
                         test_source, test_column_name, test_id, interim_table,
                         k, output_neighbors, fn_dist, weighted_avg, label_out,
                         comma_label_out_alias, label_name, train, train_id, dist_inverse)
                test_id_temp = test_id
        if not use_kdtree:
            plpy.execute("""
                CREATE TABLE {interim_table} AS
                    SELECT * FROM (
                        SELECT row_number() over
                                (partition by {test_id_temp} order by {dist}) AS {r},
                                {test_id_temp},
                                {train_id},
                                CASE WHEN {dist} = 0.0 THEN {max_weight_zero_dist}
                                     ELSE 1.0 / {dist}
                                END AS {dist_inverse}
                                {comma_label_out_alias}
                        FROM (
                            SELECT {test}.{test_id} AS {test_id_temp},
                                {train}.{point_id} as {train_id},
                                {fn_dist}(
                                    {p_col_name},
                                    {t_col_name})
                                AS {dist}
                                {label_out}
                                FROM
                                (
                                SELECT {point_id} , {point_column_name} as {p_col_name} {label_name} from {point_source}
                                ) {train},
                                (
                                SELECT {test_id} ,{test_column_name} as {t_col_name} from {test_source}
                                ) {test}
                            ) {x_temp_table}
                        ) {y_temp_table}
                WHERE {y_temp_table}.{r} <= {k}
                """.format(max_weight_zero_dist=MAX_WEIGHT_ZERO_DIST, **locals()))

        sql = """
            CREATE TABLE {output_table} AS
                {view_def}
                SELECT
                    knn_temp.{test_id_temp} AS id,
                    {test_column_name} as "{test_column_name}"
                    {pred_out}
                    {knn_neighbors}
                FROM
                    {interim_table}  AS knn_temp
                    JOIN
                    {test_source} AS knn_test
                ON knn_temp.{test_id_temp} = knn_test.{test_id}
                    {view_join}
                GROUP BY knn_temp.{test_id_temp},
                    {test_column_name}
                         {view_grp_by}
            """
        plpy.execute(sql.format(**locals()))
        plpy.execute("DROP TABLE IF EXISTS {0}".format(interim_table))
        return


def knn_help(schema_madlib, message, **kwargs):
    """
    Help function for knn

    Args:
        @param schema_madlib
        @param message: string, Help message string
        @param kwargs

    Returns:
        String. Help/usage information
    """
    if message is not None and \
            message.lower() in ("usage", "help", "?"):
        help_string = """
-----------------------------------------------------------------------
                            USAGE
-----------------------------------------------------------------------
SELECT {schema_madlib}.knn(
    point_source,       -- Training data table having training features as vector column and labels
    point_column_name,  -- Name of column having feature vectors in training data table
    point_id,           -- Name of column having feature vector Ids in train data table
    label_column_name,  -- Name of column having actual label/vlaue for corresponding feature vector in training data table
    test_source,        -- Test data table having features as vector column. Id of features is mandatory
    test_column_name,   -- Name of column having feature vectors in test data table
    test_id,     -- Name of column having feature vector Ids in test data table
    output_table,       -- Name of output table
    k,                  -- value of k. Default will go as 1
    output_neighbors    -- Outputs the list of k-nearest neighbors that were used in the voting/averaging.
    fn_dist             -- The name of the function to use to calculate the distance from a data point to a centroid.
    weighted_avg         Calculates the Regression or classication of k-NN using the weighted average method.
    );

-----------------------------------------------------------------------
                            OUTPUT
-----------------------------------------------------------------------
The output of the KNN module is a table with the following columns:

id                  The ids of test data points.
test_column_name    The test data points.
prediction          The output of KNN- label in case of classification, average value in case of regression.
k_nearest_neighbours The list of k-nearest neighbors that were used in the voting/averaging.
"""
    else:
        help_string = """
----------------------------------------------------------------------------
                                SUMMARY
----------------------------------------------------------------------------
k-Nearest Neighbors is a method for finding k closest points to a given data
point in terms of a given metric. Its input consist of data points as features
from testing examples. For a given k, it looks for k closest points in
training set for each of the data points in test set. Algorithm generates one
output per testing example. The output of KNN depends on the type of task:
For Classification, the output is majority vote of the classes of the k
nearest data points. The testing example gets assigned the most popular class
among nearest neighbors. For Regression, the output is average of the values
of k nearest neighbors of the given testing example.
--
For an overview on usage, run:
SELECT {schema_madlib}.knn('usage');
"""

    return help_string.format(schema_madlib=schema_madlib)
# ------------------------------------------------------------------------------
