# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import datetime
import numpy as np
import os
import plpy
import sys
import time

# Do not remove `import keras` although it's not directly used in this file.
# For ex if the user passes in the optimizer as keras.optimizers.SGD instead of just
# SGD, then without this import this python file won't find the SGD module
import keras

from keras import backend as K
from keras import utils as keras_utils
from keras.layers import *
from keras.models import *
from keras.optimizers import *
from keras.regularizers import *
import madlib_keras_serializer
from madlib_keras_helper import CLASS_VALUES_COLNAME
from madlib_keras_helper import DEPENDENT_VARTYPE_COLNAME
from madlib_keras_helper import expand_input_dims
from madlib_keras_helper import NORMALIZING_CONST_COLNAME
from madlib_keras_validator import FitInputValidator
from madlib_keras_wrapper import *
from keras_model_arch_table import Format

from utilities.model_arch_info import get_input_shape
from utilities.model_arch_info import get_num_classes
from utilities.utilities import is_platform_pg
from utilities.utilities import madlib_version
from utilities.validate_args import get_col_value_and_type
from utilities.validate_args import quote_ident

def fit(schema_madlib, source_table, model, dependent_varname,
        independent_varname, model_arch_table, model_arch_id, compile_params,
        fit_params, num_iterations, use_gpu = True,
        validation_table=None, name="", description="", **kwargs):

    source_table = quote_ident(source_table)
    dependent_varname = quote_ident(dependent_varname)
    independent_varname = quote_ident(independent_varname)
    model_arch_table = quote_ident(model_arch_table)

    fit_validator = FitInputValidator(
        source_table, validation_table, model, model_arch_table,
        dependent_varname, independent_varname, num_iterations)

    start_training_time = datetime.datetime.now()
    use_gpu = bool(use_gpu)

    # Get the serialized master model
    start_deserialization = time.time()
    model_arch_query = "SELECT {0}, {1} FROM {2} WHERE {3} = {4}".format(
                                        Format.MODEL_ARCH, Format.MODEL_WEIGHTS,
                                        model_arch_table, Format.MODEL_ID,
                                        model_arch_id)
    query_result = plpy.execute(model_arch_query)
    if not  query_result:
        plpy.error("no model arch found in table {0} with id {1}".format(
            model_arch_table, model_arch_id))
    query_result = query_result[0]
    model_arch = query_result[Format.MODEL_ARCH]
    input_shape = get_input_shape(model_arch)
    num_classes = get_num_classes(model_arch)
    fit_validator.validate_input_shapes(input_shape)
    model_weights_serialized = query_result[Format.MODEL_WEIGHTS]

    #TODO: Refactor the pg related logic in a future PR when we think
    # about making the fit function easier to read and maintain.
    if is_platform_pg():
        set_keras_session(use_gpu)
    else:
        # Disable GPU on master for gpdb
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

    # Compute total images on each segment
    gp_segment_id_col,\
    seg_ids_train,\
    total_images_per_seg = get_images_per_seg(source_table, dependent_varname)

    if validation_table:
        seg_ids_val, rows_per_seg_val = get_rows_per_seg_from_db(validation_table)

    # Convert model from json and initialize weights
    master_model = model_from_json(model_arch)
    model_weights = master_model.get_weights()

    # Get shape of weights in each layer from model arch
    model_shapes = []
    for weight_arr in master_model.get_weights():
        model_shapes.append(weight_arr.shape)

    if model_weights_serialized:
        # If warm start from previously trained model, set weights
        model_weights = madlib_keras_serializer.deserialize_weights_orig(
            model_weights_serialized, model_shapes)
        master_model.set_weights(model_weights)

    # Construct validation dataset if provided
    validation_set_provided = bool(validation_table)
    validation_aggregate_accuracy = []; validation_aggregate_loss = []

    total_images_per_seg = [int(each_segment["total_images_per_seg"])
        for each_segment in total_images_per_seg]

    # Prepare the SQL for running distributed training via UDA
    compile_params_to_pass = "$madlib$" + compile_params + "$madlib$"
    fit_params_to_pass = "$madlib$" + fit_params + "$madlib$"
    run_training_iteration = plpy.prepare("""
        SELECT {schema_madlib}.fit_step(
            {independent_varname}::REAL[],
            {dependent_varname}::SMALLINT[],
            {gp_segment_id_col},
            {num_classes}::INTEGER,
            ARRAY{seg_ids_train},
            ARRAY{total_images_per_seg},
            $MAD${model_arch}$MAD$::TEXT,
            {compile_params_to_pass}::TEXT,
            {fit_params_to_pass}::TEXT,
            {use_gpu},
            $1
        ) AS iteration_result
        FROM {source_table}
        """.format(**locals()), ["bytea"])

    # Define the state for the model and loss/accuracy storage lists
    model_state = madlib_keras_serializer.serialize_weights(
        0, 0, 0, model_weights)
    aggregate_loss, aggregate_accuracy, aggregate_runtime = [], [], []

    plpy.info("Model architecture size: {}KB".format(len(model_arch)/1024))
    plpy.info("Model state (serialized) size: {}MB".format(
        len(model_state)/1024/1024))

    # Run distributed training for specified number of iterations
    for i in range(num_iterations):
        start_iteration = time.time()
        iteration_result = plpy.execute(run_training_iteration, [model_state])[0]['iteration_result']
        end_iteration = time.time()
        plpy.info("Time for iteration {0}: {1} sec".
                  format(i + 1, end_iteration - start_iteration))
        aggregate_runtime.append(datetime.datetime.now())
        avg_loss, avg_accuracy, model_state = madlib_keras_serializer.deserialize_iteration_state(iteration_result)
        plpy.info("Average loss after training iteration {0}: {1}".format(
            i + 1, avg_loss))
        plpy.info("Average accuracy after training iteration {0}: {1}".format(
            i + 1, avg_accuracy))
        if validation_set_provided:
            _, _, _, updated_weights = madlib_keras_serializer.deserialize_weights(
                model_state, model_shapes)
            master_model.set_weights(updated_weights)
            start_val = time.time()
            evaluate_result = get_loss_acc_from_keras_eval(schema_madlib,
                                                           validation_table,
                                                           dependent_varname,
                                                           independent_varname,
                                                           compile_params_to_pass,
                                                           model_arch, model_state,
                                                           use_gpu, seg_ids_val,
                                                           rows_per_seg_val,
                                                           gp_segment_id_col)
            end_val = time.time()
            plpy.info("Time for validation in iteration {0}: {1} sec". format(i + 1, end_val - start_val))
            if len(evaluate_result) < 2:
                plpy.error('Calling evaluate on validation data returned < 2 '
                           'metrics. Expected metrics are loss and accuracy')
            validation_loss = evaluate_result[0]
            validation_accuracy = evaluate_result[1]
            plpy.info("Validation set accuracy after iteration {0}: {1}".
                      format(i + 1, validation_accuracy))
            validation_aggregate_accuracy.append(validation_accuracy)
            validation_aggregate_loss.append(validation_loss)
        aggregate_loss.append(avg_loss)
        aggregate_accuracy.append(avg_accuracy)

    end_training_time = datetime.datetime.now()

    final_validation_acc = None
    if validation_aggregate_accuracy and len(validation_aggregate_accuracy) > 0:
        final_validation_acc = validation_aggregate_accuracy[-1]

    final_validation_loss = None
    if validation_aggregate_loss and len(validation_aggregate_loss) > 0:
        final_validation_loss = validation_aggregate_loss[-1]
    version = madlib_version(schema_madlib)
    class_values, class_values_type = get_col_value_and_type(
        fit_validator.source_summary_table, CLASS_VALUES_COLNAME)
    norm_const, norm_const_type = get_col_value_and_type(
        fit_validator.source_summary_table, NORMALIZING_CONST_COLNAME)
    dep_vartype = plpy.execute("SELECT {0} AS dep FROM {1}".format(
        DEPENDENT_VARTYPE_COLNAME, fit_validator.source_summary_table))[0]['dep']
    create_output_summary_table = plpy.prepare("""
        CREATE TABLE {0}_summary AS
        SELECT
        $1 AS model_arch_table,
        $2 AS model_arch_id,
        $3 AS model_type,
        $4 AS start_training_time,
        $5 AS end_training_time,
        $6 AS source_table,
        $7 AS validation_table,
        $8 AS model,
        $9 AS dependent_varname,
        $10 AS independent_varname,
        $11 AS name,
        $12 AS description,
        $13 AS model_size,
        $14 AS madlib_version,
        $15 AS compile_params,
        $16 AS fit_params,
        $17 AS num_iterations,
        $18 AS num_classes,
        $19 AS accuracy,
        $20 AS loss,
        $21 AS accuracy_iter,
        $22 AS loss_iter,
        $23 AS time_iter,
        $24 AS accuracy_validation,
        $25 AS loss_validation,
        $26 AS accuracy_iter_validation,
        $27 AS loss_iter_validation,
        $28 AS {1},
        $29 AS {2},
        $30 AS {3}
        """.format(model, CLASS_VALUES_COLNAME, DEPENDENT_VARTYPE_COLNAME,
                   NORMALIZING_CONST_COLNAME),
                   ["TEXT", "INTEGER", "TEXT", "TIMESTAMP",
                    "TIMESTAMP", "TEXT", "TEXT","TEXT",
                    "TEXT", "TEXT", "TEXT", "TEXT", "INTEGER",
                    "TEXT", "TEXT", "TEXT", "INTEGER",
                    "INTEGER", "DOUBLE PRECISION",
                    "DOUBLE PRECISION", "DOUBLE PRECISION[]",
                    "DOUBLE PRECISION[]", "TIMESTAMP[]",
                    "DOUBLE PRECISION", "DOUBLE PRECISION",
                    "DOUBLE PRECISION[]", "DOUBLE PRECISION[]",
                    class_values_type, "TEXT", norm_const_type])
    plpy.execute(
        create_output_summary_table,
        [
            model_arch_table, model_arch_id,
            "madlib_keras",
            start_training_time, end_training_time,
            source_table, validation_table,
            model, dependent_varname,
            independent_varname, name, description,
            sys.getsizeof(model), version, compile_params,
            fit_params, num_iterations, num_classes,
            aggregate_accuracy[-1],
            aggregate_loss[-1],
            aggregate_accuracy, aggregate_loss,
            aggregate_runtime, final_validation_acc,
            final_validation_loss,
            validation_aggregate_accuracy,
            validation_aggregate_loss,
            class_values,
            dep_vartype,
            norm_const
        ]
        )

    create_output_table = plpy.prepare("""
        CREATE TABLE {0} AS
        SELECT $1 as model_data""".format(model), ["bytea"])
    plpy.execute(create_output_table, [model_state])

    if is_platform_pg():
        clear_keras_session()

def get_images_per_seg(source_table, dependent_varname):
    """
    Compute total images in each segment, by querying source_table.  For
    postgres, this is just the total number of images in the db.
    :param source_table:
    :param dependent_var:
    :return: Returns a string and two arrays
    1. The appropriate string to use for querying segment number
    ("gp_segment_id" for gpdb or "-1" for postgres).
    1. An array containing all the segment numbers in ascending order
    1. An array containing the total images on each of the segments in the
    segment array.
    """
    if is_platform_pg():
        total_images_per_seg = plpy.execute(
            """ SELECT SUM(ARRAY_LENGTH({0}, 1)) AS total_images_per_seg
                FROM {1}
            """.format(dependent_varname, source_table))
        seg_ids_train = "[]::integer[]"
        gp_segment_id_col =  -1
    else:
        total_images_per_seg = plpy.execute(
            """ SELECT gp_segment_id, SUM(ARRAY_LENGTH({0}, 1)) AS total_images_per_seg
                FROM {1}
                GROUP BY gp_segment_id
            """.format(dependent_varname, source_table))
        seg_ids_train = [int(each_segment["gp_segment_id"])
                   for each_segment in total_images_per_seg]
        gp_segment_id_col = 'gp_segment_id'
    return gp_segment_id_col, seg_ids_train, total_images_per_seg
 
def get_rows_per_seg_from_db(table_name):
    """
    This function queries the given table and returns the total rows per segment.
    Since we cannot pass a dictionary to the keras fit step function we create arrays
    out of the segment numbers and the rows per segment values.
    This function assumes that the table is not empty.
    :param table_name:
    :return: Returns two arrays
    1. An array containing all the segment numbers in ascending order
    1. An array containing the total rows for each of the segments in the
    segment array
    """
    if is_platform_pg():
        rows = plpy.execute(
            """ SELECT count(*) AS rows_per_seg
                FROM {0}
            """.format(table_name))
        seg_ids = "[]::integer[]"
    else:
        # Compute total buffers on each segment
        rows = plpy.execute(
            """ SELECT gp_segment_id, count(*) AS rows_per_seg
                FROM {0}
                GROUP BY gp_segment_id
            """.format(table_name))
        seg_ids = [int(row["gp_segment_id"]) for row in rows]

    rows = [int(row["rows_per_seg"]) for row in rows]
    return seg_ids, rows


def fit_transition(state, ind_var, dep_var, current_seg_id, num_classes,
                   all_seg_ids, total_images_per_seg, architecture,
                   compile_params, fit_params, use_gpu, previous_state,
                   **kwargs):

    """

    :param state:
    :param ind_var:
    :param dep_var:
    :param current_seg_id:
    :param num_classes:
    :param all_seg_ids:
    :param total_images_per_seg:
    :param architecture:
    :param compile_params:
    :param fit_params:
    :param use_gpu:
    :param previous_state:
    :param kwargs:
    :return:
    """
    if not ind_var or not dep_var:
        return state

    start_transition = time.time()
    SD = kwargs['SD']
    # Configure GPUs/CPUs
    device_name = get_device_name_and_set_cuda_env(use_gpu, current_seg_id)

    # Set up system if this is the first buffer on segment'

    if not state:
        if not is_platform_pg():
            set_keras_session(use_gpu)
        segment_model = model_from_json(architecture)
        SD['model_shapes'] = madlib_keras_serializer.get_model_shapes(segment_model)
        compile_and_set_weights(segment_model, compile_params, device_name,
                                previous_state, SD['model_shapes'])
        SD['segment_model'] = segment_model
        image_count = 0
        agg_loss = 0
        agg_accuracy = 0
        agg_image_count = 0
    else:
        segment_model = SD['segment_model']
        #TODO we don't need to deserialize the weights here.
        agg_loss, agg_accuracy, agg_image_count, _ = madlib_keras_serializer.deserialize_weights(
            state, SD['model_shapes'])

    # Prepare the data
    x_train = np.array(ind_var, dtype='float64')
    y_train = np.array(dep_var)

    # Fit segment model on data
    start_fit = time.time()
    with K.tf.device(device_name):
        #TODO consider not doing this every time
        fit_params = parse_fit_params(fit_params)
        history = segment_model.fit(x_train, y_train, **fit_params)
        loss = history.history['loss'][0]
        accuracy = history.history['acc'][0]
    end_fit = time.time()

    image_count = len(x_train)
    # Aggregating number of images, loss and accuracy
    agg_image_count += image_count
    agg_loss += (image_count * loss)
    agg_accuracy += (image_count * accuracy)

    with K.tf.device(device_name):
        updated_weights = segment_model.get_weights()
    if is_platform_pg():
        total_images = total_images_per_seg[0]
    else:
        total_images = total_images_per_seg[all_seg_ids.index(current_seg_id)]
    if total_images == 0:
        plpy.error('Got 0 rows. Expected at least 1.')

    # Re-serialize the weights
    # Update image count, check if we are done
    if agg_image_count == total_images:
        if total_images == 0:
            plpy.error('Total images is 0')
        # Once done with all images on a segment, we update weights
        # with the total number of images here instead of the merge function.
        # The merge function only deals with aggregating them.
        updated_weights = [ total_images * w for w in updated_weights ]
        if not is_platform_pg():
            # In GPDB, each segment would have a keras session, so clear
            # them after the last buffer is processed.
            clear_keras_session()
    elif agg_image_count > total_images:
        plpy.error('Processed {0} images, but there were supposed to be only {1}!'
            .format(agg_image_count,total_images))

    new_model_state = madlib_keras_serializer.serialize_weights(
            agg_loss, agg_accuracy, agg_image_count, updated_weights)


    del x_train
    del y_train

    end_transition = time.time()
    plpy.info("Processed {0} images: Fit took {1} sec, Total was {2} sec".format(
        image_count, end_fit - start_fit, end_transition - start_transition))

    return new_model_state

def fit_merge(state1, state2, **kwargs):

    # Return if called early
    if not state1 or not state2:
        return state1 or state2

    # Deserialize states
    loss1, accuracy1, image_count1, weights1 = madlib_keras_serializer.deserialize_weights_merge(state1)
    loss2, accuracy2, image_count2, weights2 = madlib_keras_serializer.deserialize_weights_merge(state2)

    # Compute total image counts
    image_count = (image_count1 + image_count2) * 1.0
    plpy.info("FIT_MERGE: Mergeing {0} + {1} = {2} images".format(image_count1,image_count2,image_count))
    if image_count == 0:
        plpy.error('total images in merge is 0')

    # Aggregate the losses
    total_loss = loss1 + loss2

    # Aggregate the accuracies
    total_accuracy = accuracy1 + accuracy2

    # Aggregate the weights
    total_weights = weights1 + weights2

    # Return the merged state
    return madlib_keras_serializer.serialize_weights_merge(
        total_loss, total_accuracy, image_count, total_weights)

def fit_final(state, **kwargs):
    # Return if called early
    if not state:
        return state

    loss, accuracy, image_count, weights = madlib_keras_serializer.deserialize_weights_merge(state)
    # Averaging the accuracy, loss and weights
    loss /= image_count
    accuracy /= image_count
    weights /= image_count
    return madlib_keras_serializer.serialize_weights_merge(
        loss, accuracy, image_count, weights)

def evaluate1(schema_madlib, model_table, test_table, id_col, model_arch_table,
            model_arch_id, dependent_varname, independent_varname,
            compile_params, output_table, **kwargs):
    # module_name = 'madlib_keras_evaluate'
    # input_tbl_valid(test_table, module_name)
    # input_tbl_valid(model_arch_table, module_name)
    # output_tbl_valid(output_table, module_name)

    # _validate_input_args(test_table, model_arch_table, output_table)

    model_data_query = "SELECT model_data from {0}".format(model_table)
    model_data = plpy.execute(model_data_query)[0]['model_data']

    model_arch_query = "SELECT model_arch, model_weights FROM {0} " \
                       "WHERE id = {1}".format(model_arch_table, model_arch_id)
    query_result = plpy.execute(model_arch_query)
    if not  query_result or len(query_result) == 0:
        plpy.error("no model arch found in table {0} with id {1}".format(
            model_arch_table, model_arch_id))
    query_result = query_result[0]
    model_arch = query_result[Format.MODEL_ARCH]
    compile_params = "$madlib$" + compile_params + "$madlib$"

    loss_acc = get_loss_acc_from_keras_eval(schema_madlib, test_table, dependent_varname,
                                            independent_varname, compile_params, model_arch,
                                            model_data, False)
    #TODO remove these infos after adding create table command
    plpy.info('len of evaluate result is {}'.format(len(loss_acc)))
    plpy.info('evaluate result loss is {}'.format(loss_acc[0]))
    plpy.info('evaluate result acc is {}'.format(loss_acc[1]))

def get_loss_acc_from_keras_eval(schema_madlib, table, dependent_varname,
                                 independent_varname, compile_params, model_arch,
                                 model_data, use_gpu, seg_ids_val,
                                 rows_per_seg_val, gp_segment_id_col):
    """
    This function will call the internal keras evaluate function to get the loss
    and accuracy of each tuple which then gets averaged to get the final result.
    """
    evaluate_query = plpy.prepare("""
    select {schema_madlib}.array_avg(loss_acc, True) as final_loss_acc from
    (
        select ({schema_madlib}.internal_keras_evaluate({dependent_varname},
                                            {independent_varname},
                                            $MAD${model_arch}$MAD$,
                                            $1, {compile_params},
                                            {use_gpu}, 
                                            ARRAY{seg_ids_val}, 
                                            ARRAY{rows_per_seg_val},
                                            {gp_segment_id_col})) as loss_acc 
        from {table}
    ) q""".format(**locals()), ["bytea"])
    res = plpy.execute(evaluate_query, [model_data])
    loss_acc = res[0]['final_loss_acc']
    return loss_acc


def internal_keras_evaluate(dependent_var, independent_var, model_architecture,
                            model_data, compile_params, use_gpu, seg_ids_val,
                            rows_per_seg_val, current_seg, **kwargs):
    SD = kwargs['SD']
    device_name = get_device_name_and_set_cuda_env(use_gpu, current_seg)

    if 'segment_model' not in SD:
        if not is_platform_pg():
            set_keras_session(use_gpu)
        model = model_from_json(model_architecture)
        model_shapes = madlib_keras_serializer.get_model_shapes(model)
        _, _, _, model_weights = madlib_keras_serializer.deserialize_weights(
            model_data, model_shapes)
        model.set_weights(model_weights)
        with K.tf.device(device_name):
            compile_model(model, compile_params)
        SD['segment_model'] = model
        SD['row_count'] = 0
    else:
        model = SD['segment_model']
    SD['row_count'] += 1

    # Since the training data is batched but the validation data isn't,
    # we have to make sure that the validation data np array has the same
    # number of dimensions as training data. So we prepend a dimension to
    # both x and y np arrays using expand_dims.
    independent_var = expand_input_dims(independent_var, target_type='float32')
    dependent_var = expand_input_dims(dependent_var)

    with K.tf.device(device_name):
        res = model.evaluate(independent_var, dependent_var)
    if is_platform_pg():
        total_rows = rows_per_seg_val[0]
    else:
        total_rows = rows_per_seg_val[seg_ids_val.index(current_seg)]

    if is_last_row_in_seg(SD['row_count'], total_rows):
        SD.pop('segment_model', None)
        if not is_platform_pg():
            clear_keras_session()

    return res


def is_last_row_in_seg(row_count, total_rows):
    return row_count == total_rows
